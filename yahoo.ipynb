{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "yahoo",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npKdnlDKlq8U"
      },
      "source": [
        "## Real Time Stock Market Recommender Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swt4QJfulq8a"
      },
      "source": [
        "### Streaming data using kafka"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zczp6-CIlq8d",
        "outputId": "3c4fd176-6e17-48ba-950e-aa957ae83b15"
      },
      "source": [
        "import pandas as pd\n",
        "from kafka import KafkaProducer\n",
        "KAFKA_TOPIC_NAME_CONS = \"stockTopic\"\n",
        "KAFKA_BOOTSTRAP_SERVERS_CONS = 'localhost:9092' \n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Kafka Producer Application Started ... \")\n",
        "    kafka_producer_obj = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS_CONS,value_serializer=lambda x: x.encode('utf-8'))\n",
        "    filepath = \"yahoo.csv\"\n",
        "    df = pd.read_csv(filepath)\n",
        "    list = df.to_dict(orient=\"records\")\n",
        "    message_list = []\n",
        "    message = None\n",
        "    for message in list:\n",
        "        message_fields_value_list = []\n",
        "        message_fields_value_list.append(message[\"Date\"])\n",
        "        message_fields_value_list.append(message[\"Open\"])\n",
        "        message_fields_value_list.append(message[\"High\"])\n",
        "        message_fields_value_list.append(message[\"Low\"])\n",
        "        message_fields_value_list.append(message[\"Close\"])\n",
        "        message_fields_value_list.append(message[\"Adj Close\"])\n",
        "        message_fields_value_list.append(message[\"Volume\"])\n",
        "        message = ','.join(str(v) for v in message_fields_value_list)\n",
        "        print(\"Message Type: \", type(message))\n",
        "        print(\"Message: \", message)\n",
        "        kafka_producer_obj.send(KAFKA_BOOTSTRAP_SERVERS_CONS, message)\n",
        "        time.sleep(1)\n",
        "        \n",
        "    print(\"Kafka Producer Application Completed. \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kafka Producer Application Started ... \n",
            "Message Type:  <class 'str'>\n",
            "Message:  1996-10-01,19.25,19.625,19.0,19.25,17.329784,54000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KafkaTimeoutError",
          "evalue": "KafkaTimeoutError: Failed to update metadata after 60.0 secs.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKafkaTimeoutError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-bc325a4400e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Message Type: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Message: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mkafka_producer_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKAFKA_BOOTSTRAP_SERVERS_CONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\producer\\kafka.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[0mkey_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_on_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_block_ms'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             key_bytes = self._serialize(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\kafka\\producer\\kafka.py\u001b[0m in \u001b[0;36m_wait_on_metadata\u001b[1;34m(self, topic, max_wait)\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[0melapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmetadata_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m                 raise Errors.KafkaTimeoutError(\n\u001b[0m\u001b[0;32m    703\u001b[0m                     \"Failed to update metadata after %.1f secs.\" % (max_wait,))\n\u001b[0;32m    704\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munauthorized_topics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKafkaTimeoutError\u001b[0m: KafkaTimeoutError: Failed to update metadata after 60.0 secs."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0p3oRUKlq8i"
      },
      "source": [
        "### Installing PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwB7NtXplq8j",
        "outputId": "693aff8f-1f7e-47f6-c8f7-8cc622841192"
      },
      "source": [
        "conda install -c conda-forge findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: C:\\Users\\sreer\\anaconda3\n",
            "\n",
            "  added / updated specs:\n",
            "    - findspark\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  python_abi         conda-forge/win-64::python_abi-3.8-1_cp38\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  conda              pkgs/main::conda-4.10.1-py38haa95532_1 --> conda-forge::conda-4.10.1-py38haa244fe_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7DrSHXclq8k",
        "outputId": "88e2a8d4-c95f-422d-d8b4-fc18f8b69065"
      },
      "source": [
        "conda update -n base -c defaults conda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: C:\\Users\\sreer\\anaconda3\n",
            "\n",
            "  added / updated specs:\n",
            "    - conda\n",
            "\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  python_abi-3.8-1_cp38\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  conda              conda-forge::conda-4.10.1-py38haa244f~ --> pkgs/main::conda-4.10.1-py38haa95532_1\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkQB40Flq8m",
        "outputId": "ab613491-048f-4199-fd6f-75556561486d"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\opt\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyPKUVdglq8p",
        "outputId": "2ce7bd97-05ec-45d8-ae8d-09c30eb8c2c0"
      },
      "source": [
        "import pyspark\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\opt\\\\spark\\\\spark-3.1.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "700CM632lq8r"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "conf = pyspark.SparkConf().setAppName('yahoo').setMaster('local')\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pcNnEGilq8u"
      },
      "source": [
        "#Loading Data\n",
        "data = spark.read.csv(\"yahoo.csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knaEunZYlq8v",
        "outputId": "66bf5a40-4504-4757-d3db-6a82b4e00b6b"
      },
      "source": [
        "data.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------+------+------+------+---------+------+\n",
            "|      Date|  Open|  High|   Low| Close|Adj Close|Volume|\n",
            "+----------+------+------+------+------+---------+------+\n",
            "|1996-10-01| 19.25|19.625|  19.0| 19.25|17.329784| 54000|\n",
            "|1996-10-02| 19.25| 19.75| 19.25|19.625|17.667372| 10000|\n",
            "|1996-10-03| 19.75| 19.75| 19.25| 19.25|17.329784| 19300|\n",
            "|1996-10-04| 19.75| 19.75| 19.25| 19.75|17.779911| 45400|\n",
            "|1996-10-07|19.375|20.375|19.375|20.375|18.342564| 41500|\n",
            "+----------+------+------+------+------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-a971B0lq8v",
        "outputId": "19db2272-4b07-4419-d5c4-3a1349505362"
      },
      "source": [
        "#Structure of data\n",
        "data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X33f6J7Blq8x",
        "outputId": "0dbf6d70-69f5-4357-8bc5-bf6a5f522311"
      },
      "source": [
        "#Number of rows\n",
        "data.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvNvr869lq8y",
        "outputId": "430e0ad9-b063-48e3-e6b7-a8f8b2907b78"
      },
      "source": [
        "#Displaying Columns\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8zfN1bMlq8z"
      },
      "source": [
        "### Finding Null Values if Any"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI90TybIlq80",
        "outputId": "a4e3b316-aad4-4119-831d-940e54cd96df"
      },
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----+----+---+-----+---------+------+\n",
            "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
            "+----+----+----+---+-----+---------+------+\n",
            "|   0|   0|   0|  0|    0|        0|     0|\n",
            "+----+----+----+---+-----+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYOkC2G2lq81",
        "outputId": "4f64222b-a408-4fc7-c7a1-bf1578b97258"
      },
      "source": [
        "#Identifying string column\n",
        "strCols=[item[0] for item in data.dtypes if item[1].startswith('string')]\n",
        "strCols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_iNg81Jlq82"
      },
      "source": [
        "data=data.drop(*['Date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjitdXOlq82"
      },
      "source": [
        "### Vector Assembler - Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRw_DTJflq82",
        "outputId": "3db2dead-127b-491f-d29f-03fa41f74f24"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "X=data.drop(*['VWAP'])\n",
        "vectorAssembler = VectorAssembler(inputCols=X.columns, outputCol = 'features')\n",
        "v_data= vectorAssembler.transform(data)\n",
        "v_data.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+------+------+---------+------+--------------------+\n",
            "|  Open|  High|   Low| Close|Adj Close|Volume|            features|\n",
            "+------+------+------+------+---------+------+--------------------+\n",
            "| 19.25|19.625|  19.0| 19.25|17.329784| 54000|[19.25,19.625,19....|\n",
            "| 19.25| 19.75| 19.25|19.625|17.667372| 10000|[19.25,19.75,19.2...|\n",
            "| 19.75| 19.75| 19.25| 19.25|17.329784| 19300|[19.75,19.75,19.2...|\n",
            "| 19.75| 19.75| 19.25| 19.75|17.779911| 45400|[19.75,19.75,19.2...|\n",
            "|19.375|20.375|19.375|20.375|18.342564| 41500|[19.375,20.375,19...|\n",
            "+------+------+------+------+---------+------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8dFPtGnlq83"
      },
      "source": [
        "### StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EihNrtBNlq84",
        "outputId": "84b17bf1-6963-4a6d-8e2c-b18d1809715b"
      },
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol='features', outputCol=\"scaledFeatures\", withStd=False, withMean=True)\n",
        "scalerModel = scaler.fit(v_data)\n",
        "scaledData = scalerModel.transform(v_data)\n",
        "scaledData.select(['scaledFeatures']).show(5, truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------+\n",
            "|scaledFeatures                                                                                                  |\n",
            "+----------------------------------------------------------------------------------------------------------------+\n",
            "|[6.267681009321739,6.458783556412733,6.2032797965284345,6.267034314850514,5.208434203310778,-13413.596914175607]|\n",
            "|[6.267681009321739,6.583783556412733,6.4532797965284345,6.642034314850514,5.546022203310779,-57413.59691417561] |\n",
            "|[6.767681009321739,6.583783556412733,6.4532797965284345,6.267034314850514,5.208434203310778,-48113.59691417561] |\n",
            "|[6.767681009321739,6.583783556412733,6.4532797965284345,6.767034314850514,5.658561203310777,-22013.596914175607]|\n",
            "|[6.392681009321739,7.208783556412733,6.5782797965284345,7.392034314850514,6.221214203310778,-25913.596914175607]|\n",
            "+----------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qEq7ODplq85"
      },
      "source": [
        "### PCA- Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNMrf3VNlq85",
        "outputId": "3e5b355f-bc00-419d-cd85-222301c5daca"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "pca = PCA(k=2, inputCol = scaler.getOutputCol(), outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(scaledData)\n",
        "transformed_feature = model.transform(scaledData)\n",
        "transformed_feature.select(['pcaFeatures']).show(5, truncate = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------+\n",
            "|pcaFeatures                             |\n",
            "+----------------------------------------+\n",
            "|[13413.596069293028,-14.421448068458078]|\n",
            "|[57413.59596010959,-17.558389599284773] |\n",
            "|[48113.595982613086,-16.907903720113996]|\n",
            "|[22013.596004577776,-15.754070142834886]|\n",
            "|[25913.595955007295,-16.685433062001366]|\n",
            "+----------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAlLuYeolq86"
      },
      "source": [
        "## Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk5JhnW3lq86"
      },
      "source": [
        "### As, the target column 'High' is numerical type, this is a Regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnzCLeeQlq87"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSlaph31lq87",
        "outputId": "a7d4fd75-aed6-4c05-b859-2a5b943ea894"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "trainTest = v_data.randomSplit([0.5, 0.5])\n",
        "trainingDF = trainTest[0]\n",
        "testDF = trainTest[1]\n",
        "lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8).setFeaturesCol('features').setLabelCol('High')\n",
        "model = lir.fit(trainingDF)\n",
        "print(\"Coefficients: \" + str(model.coefficients))\n",
        "print(\"Intercept: \" + str(model.intercept))\n",
        "trainingSummary = model.summary\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)\n",
        "fullPredictions = model.transform(testDF).cache()\n",
        "print(fullPredictions)\n",
        "fullPredictions.show(5)\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"High\",metricName=\"r2\")\n",
        "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(fullPredictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: [0.1996344038993007,0.20427241960720058,0.19864979943991856,0.1983754856197377,0.1875054051309425,0.0]\n",
            "Intercept: 0.49484103143261166\n",
            "RMSE: 0.320786\n",
            "r2: 0.998789\n",
            "DataFrame[Open: double, High: double, Low: double, Close: double, Adj Close: double, Volume: int, features: vector, prediction: double]\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|  Open|  High|   Low| Close|Adj Close|Volume|            features|        prediction|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|1.1875|1.3125| 1.125|1.1875| 1.069045| 87300|[1.1875,1.3125,1....|1.6595180661690372|\n",
            "|1.3125|1.5625|1.3125|   1.5| 1.350373| 17700|[1.3125,1.5625,1....|1.8875301688240809|\n",
            "|1.3125| 1.625|1.3125|1.5625| 1.406639|140800|[1.3125,1.625,1.3...| 1.923245842025862|\n",
            "| 1.375| 1.375|1.0625|1.1875| 1.069045|153600|[1.375,1.375,1.06...|1.6973009306606115|\n",
            "| 1.375|1.4375|1.3125|1.4375| 1.294107| 30800|[1.375,1.4375,1.3...|1.8515246196405557|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "R Squared (R2) on test data = 0.998812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfPfz8bBlq88"
      },
      "source": [
        "### RandomForest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Eg2qZR2lq89",
        "outputId": "bdc9be55-2b13-4fe7-8636-213ce349ef2e"
      },
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "rf = RandomForestRegressor(featuresCol=\"features\",impurity='gini',maxDepth=5,numTrees=30)\n",
        "predictions = model.transform(testDF)\n",
        "predictions.show(5)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"High\",metricName=\"r2\")\n",
        "print(\"R Squared (R2) on test data = %g\" % rf_evaluator.evaluate(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|  Open|  High|   Low| Close|Adj Close|Volume|            features|        prediction|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|1.1875|1.3125| 1.125|1.1875| 1.069045| 87300|[1.1875,1.3125,1....|1.6595180661690372|\n",
            "|1.3125|1.5625|1.3125|   1.5| 1.350373| 17700|[1.3125,1.5625,1....|1.8875301688240809|\n",
            "|1.3125| 1.625|1.3125|1.5625| 1.406639|140800|[1.3125,1.625,1.3...| 1.923245842025862|\n",
            "| 1.375| 1.375|1.0625|1.1875| 1.069045|153600|[1.375,1.375,1.06...|1.6973009306606115|\n",
            "| 1.375|1.4375|1.3125|1.4375| 1.294107| 30800|[1.375,1.4375,1.3...|1.8515246196405557|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "R Squared (R2) on test data = 0.998812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8n0d6utlq8-"
      },
      "source": [
        "### Gradient-Boosted Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JB17NGVlq8-",
        "outputId": "f89ac4d9-d07c-4372-9b91-82c8d8ce7f04"
      },
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "gbt = GBTRegressor(featuresCol=\"features\", maxIter=10)\n",
        "predictions = model.transform(testDF)\n",
        "predictions.show(5)\n",
        "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"High\",metricName=\"r2\")\n",
        "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|  Open|  High|   Low| Close|Adj Close|Volume|            features|        prediction|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "|1.1875|1.3125| 1.125|1.1875| 1.069045| 87300|[1.1875,1.3125,1....|1.6595180661690372|\n",
            "|1.3125|1.5625|1.3125|   1.5| 1.350373| 17700|[1.3125,1.5625,1....|1.8875301688240809|\n",
            "|1.3125| 1.625|1.3125|1.5625| 1.406639|140800|[1.3125,1.625,1.3...| 1.923245842025862|\n",
            "| 1.375| 1.375|1.0625|1.1875| 1.069045|153600|[1.375,1.375,1.06...|1.6973009306606115|\n",
            "| 1.375|1.4375|1.3125|1.4375| 1.294107| 30800|[1.375,1.4375,1.3...|1.8515246196405557|\n",
            "+------+------+------+------+---------+------+--------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "R Squared (R2) on test data = 0.998812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PecbqTbJlq8_"
      },
      "source": [
        "### Clustering- KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGyXWDTolq9A",
        "outputId": "f361f42a-d561-4752-c46b-5c870440b98d"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(v_data)\n",
        "predictions = model.transform(v_data)\n",
        "evaluator = ClusteringEvaluator()\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.8397873255490698\n",
            "Cluster Centers: \n",
            "[1.24325350e+01 1.25910137e+01 1.22665241e+01 1.24287798e+01\n",
            " 1.15773071e+01 4.67322499e+04]\n",
            "[1.74336404e+01 1.78233406e+01 1.70894481e+01 1.74699269e+01\n",
            " 1.65261871e+01 2.34859942e+05]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}